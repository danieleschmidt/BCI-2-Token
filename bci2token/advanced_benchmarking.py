"""
Advanced Benchmarking and Validation Framework - Generation 4 Enhancement
BCI-2-Token: Comprehensive Testing and Performance Analysis

This module implements advanced benchmarking capabilities including:
- Multi-dimensional performance analysis
- Statistical significance testing with effect sizes
- A/B testing framework for model comparisons
- Adversarial robustness evaluation
- Real-time performance monitoring
- Benchmark reproducibility and versioning
"""

import numpy as np
import time
import json
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Callable, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
from abc import ABC, abstractmethod
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import warnings
from enum import Enum
import statistics

# Configure logging
logger = logging.getLogger(__name__)


class BenchmarkMetric(Enum):
    """Available benchmark metrics"""
    ACCURACY = "accuracy"
    PRECISION = "precision"
    RECALL = "recall"
    F1_SCORE = "f1_score"
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    MEMORY_USAGE = "memory_usage"
    CPU_UTILIZATION = "cpu_utilization"
    ROBUSTNESS = "robustness"
    FAIRNESS = "fairness"
    INTERPRETABILITY = "interpretability"


@dataclass
class BenchmarkConfig:
    """Configuration for benchmark experiments"""
    metrics: List[BenchmarkMetric] = field(default_factory=lambda: [
        BenchmarkMetric.ACCURACY, BenchmarkMetric.LATENCY, BenchmarkMetric.ROBUSTNESS
    ])
    num_trials: int = 10
    confidence_level: float = 0.95
    significance_threshold: float = 0.05
    effect_size_threshold: float = 0.2
    warmup_trials: int = 3
    timeout_seconds: int = 300
    parallel_execution: bool = True
    max_workers: int = 4


@dataclass
class StatisticalTestConfig:
    """Configuration for statistical testing"""
    test_type: str = "welch_t_test"  # welch_t_test, mann_whitney_u, bootstrap
    bootstrap_samples: int = 10000
    bonferroni_correction: bool = True
    multiple_comparison_method: str = "holm"  # holm, bonferroni, fdr_bh
    min_sample_size: int = 30
    power_analysis: bool = True


@dataclass 
class AdversarialConfig:
    """Configuration for adversarial testing"""
    attack_types: List[str] = field(default_factory=lambda: [
        "gaussian_noise", "uniform_noise", "signal_dropout", "temporal_shift"
    ])
    noise_levels: List[float] = field(default_factory=lambda: [0.1, 0.2, 0.5, 1.0])
    dropout_rates: List[float] = field(default_factory=lambda: [0.1, 0.25, 0.5])
    temporal_shifts: List[int] = field(default_factory=lambda: [5, 10, 20])
    num_samples_per_attack: int = 100


class BenchmarkResult:
    """Container for benchmark results with statistical analysis"""
    
    def __init__(self, model_name: str, metric: BenchmarkMetric):
        self.model_name = model_name
        self.metric = metric
        self.raw_scores = []
        self.metadata = {}
        self.timestamp = time.time()
        
    def add_score(self, score: float, metadata: Optional[Dict] = None):
        """Add a benchmark score"""
        self.raw_scores.append(score)
        if metadata:
            self.metadata[len(self.raw_scores) - 1] = metadata
    
    def get_statistics(self) -> Dict[str, float]:
        """Compute comprehensive statistics"""
        if not self.raw_scores:
            return {}
        
        scores = np.array(self.raw_scores)
        
        stats = {
            'mean': float(np.mean(scores)),
            'median': float(np.median(scores)),
            'std': float(np.std(scores, ddof=1)),
            'min': float(np.min(scores)),
            'max': float(np.max(scores)),
            'q25': float(np.percentile(scores, 25)),
            'q75': float(np.percentile(scores, 75)),
            'iqr': float(np.percentile(scores, 75) - np.percentile(scores, 25)),
            'cv': float(np.std(scores, ddof=1) / np.mean(scores)) if np.mean(scores) != 0 else 0,
            'skewness': float(statistics.pstdev(scores) / (statistics.mean(scores) ** (1/3))) if len(scores) > 2 else 0,
            'count': len(scores)
        }
        
        # Confidence intervals
        if len(scores) > 1:
            sem = stats['std'] / np.sqrt(len(scores))
            t_critical = 1.96  # For 95% confidence (approximation)
            stats['ci_lower'] = stats['mean'] - t_critical * sem
            stats['ci_upper'] = stats['mean'] + t_critical * sem
            stats['margin_of_error'] = t_critical * sem
        
        return stats
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            'model_name': self.model_name,
            'metric': self.metric.value,
            'raw_scores': self.raw_scores,
            'statistics': self.get_statistics(),
            'metadata': self.metadata,
            'timestamp': self.timestamp
        }


class StatisticalAnalyzer:
    """Advanced statistical analysis for benchmark results"""
    
    def __init__(self, config: StatisticalTestConfig):
        self.config = config
        
    def compare_models(self, results1: BenchmarkResult, 
                      results2: BenchmarkResult) -> Dict[str, Any]:
        """Compare two models using statistical tests"""
        
        if results1.metric != results2.metric:
            raise ValueError("Cannot compare results for different metrics")
        
        scores1 = np.array(results1.raw_scores)
        scores2 = np.array(results2.raw_scores)
        
        if len(scores1) < 2 or len(scores2) < 2:
            return {'error': 'Insufficient data for comparison'}
        
        comparison = {
            'model1': results1.model_name,
            'model2': results2.model_name,
            'metric': results1.metric.value,
            'sample_sizes': {'model1': len(scores1), 'model2': len(scores2)},
            'descriptive_stats': {
                'model1': results1.get_statistics(),
                'model2': results2.get_statistics()
            }
        }
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt((np.var(scores1, ddof=1) + np.var(scores2, ddof=1)) / 2)
        if pooled_std > 0:
            cohens_d = (np.mean(scores1) - np.mean(scores2)) / pooled_std
            comparison['effect_size'] = {
                'cohens_d': float(cohens_d),
                'interpretation': self._interpret_effect_size(abs(cohens_d))
            }
        
        # Statistical test
        if self.config.test_type == "welch_t_test":
            test_result = self._welch_t_test(scores1, scores2)
        elif self.config.test_type == "mann_whitney_u":
            test_result = self._mann_whitney_u_test(scores1, scores2)
        elif self.config.test_type == "bootstrap":
            test_result = self._bootstrap_test(scores1, scores2)
        else:
            test_result = {'error': f'Unknown test type: {self.config.test_type}'}
        
        comparison['statistical_test'] = test_result
        
        # Practical significance
        if 'effect_size' in comparison and 'p_value' in test_result:
            comparison['practical_significance'] = {
                'statistically_significant': test_result['p_value'] < self.config.significance_threshold,
                'practically_significant': abs(comparison['effect_size']['cohens_d']) > self.config.effect_size_threshold,
                'recommendation': self._make_recommendation(test_result['p_value'], comparison['effect_size']['cohens_d'])
            }
        
        # Power analysis
        if self.config.power_analysis:
            comparison['power_analysis'] = self._power_analysis(scores1, scores2)
        
        return comparison
    
    def _welch_t_test(self, scores1: np.ndarray, scores2: np.ndarray) -> Dict[str, Any]:
        """Welch's t-test for unequal variances"""
        n1, n2 = len(scores1), len(scores2)
        mean1, mean2 = np.mean(scores1), np.mean(scores2)
        var1, var2 = np.var(scores1, ddof=1), np.var(scores2, ddof=1)
        
        # Standard error of difference
        se = np.sqrt(var1/n1 + var2/n2)
        
        if se == 0:
            return {'error': 'Cannot compute t-test: zero standard error'}
        
        # t-statistic
        t_stat = (mean1 - mean2) / se
        
        # Degrees of freedom (Welch-Satterthwaite equation)
        df = (var1/n1 + var2/n2)**2 / ((var1/n1)**2/(n1-1) + (var2/n2)**2/(n2-1))
        
        # Approximate p-value (simplified)
        p_value = 2 * (1 - self._approximate_t_cdf(abs(t_stat), df))
        
        return {
            'test_name': 'Welch\'s t-test',
            't_statistic': float(t_stat),
            'degrees_of_freedom': float(df),
            'p_value': float(p_value),
            'significant': p_value < self.config.significance_threshold
        }
    
    def _mann_whitney_u_test(self, scores1: np.ndarray, scores2: np.ndarray) -> Dict[str, Any]:
        """Mann-Whitney U test (non-parametric)"""
        n1, n2 = len(scores1), len(scores2)
        
        # Combine and rank
        combined = np.concatenate([scores1, scores2])
        ranks = np.argsort(np.argsort(combined)) + 1  # Ranks starting from 1
        
        # Sum of ranks for first group
        r1 = np.sum(ranks[:n1])
        
        # U statistics
        u1 = r1 - n1 * (n1 + 1) / 2
        u2 = n1 * n2 - u1
        
        u_stat = min(u1, u2)
        
        # Normal approximation for p-value
        mean_u = n1 * n2 / 2
        std_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
        
        if std_u > 0:
            z_score = (u_stat - mean_u) / std_u
            p_value = 2 * (1 - self._approximate_normal_cdf(abs(z_score)))
        else:
            p_value = 1.0
        
        return {
            'test_name': 'Mann-Whitney U test',
            'u_statistic': float(u_stat),
            'z_score': float(z_score) if std_u > 0 else 0.0,
            'p_value': float(p_value),
            'significant': p_value < self.config.significance_threshold
        }
    
    def _bootstrap_test(self, scores1: np.ndarray, scores2: np.ndarray) -> Dict[str, Any]:
        """Bootstrap hypothesis test"""
        observed_diff = np.mean(scores1) - np.mean(scores2)
        
        # Pool samples for null hypothesis
        pooled = np.concatenate([scores1, scores2])
        n1, n2 = len(scores1), len(scores2)
        
        # Bootstrap resampling
        bootstrap_diffs = []
        for _ in range(self.config.bootstrap_samples):
            # Resample from pooled distribution
            resampled = np.random.choice(pooled, size=len(pooled), replace=True)
            boot_sample1 = resampled[:n1]
            boot_sample2 = resampled[n1:n1+n2]
            
            boot_diff = np.mean(boot_sample1) - np.mean(boot_sample2)
            bootstrap_diffs.append(boot_diff)
        
        bootstrap_diffs = np.array(bootstrap_diffs)
        
        # Two-tailed p-value
        p_value = np.mean(np.abs(bootstrap_diffs) >= np.abs(observed_diff))
        
        return {
            'test_name': 'Bootstrap test',
            'observed_difference': float(observed_diff),
            'bootstrap_samples': self.config.bootstrap_samples,
            'p_value': float(p_value),
            'significant': p_value < self.config.significance_threshold,
            'bootstrap_ci': {
                'lower': float(np.percentile(bootstrap_diffs, 2.5)),
                'upper': float(np.percentile(bootstrap_diffs, 97.5))
            }
        }
    
    def _power_analysis(self, scores1: np.ndarray, scores2: np.ndarray) -> Dict[str, Any]:
        """Statistical power analysis"""
        n1, n2 = len(scores1), len(scores2)
        
        # Effect size
        pooled_std = np.sqrt((np.var(scores1, ddof=1) + np.var(scores2, ddof=1)) / 2)
        if pooled_std > 0:
            effect_size = abs(np.mean(scores1) - np.mean(scores2)) / pooled_std
        else:
            effect_size = 0
        
        # Simplified power calculation (assuming equal sample sizes)
        n_harmonic = 2 * n1 * n2 / (n1 + n2)  # Harmonic mean of sample sizes
        
        # Cohen's power calculation approximation
        delta = effect_size * np.sqrt(n_harmonic / 2)
        
        # Approximate power (very simplified)
        if delta > 2.8:
            power = 0.8
        elif delta > 2.0:
            power = 0.6
        elif delta > 1.0:
            power = 0.4
        else:
            power = 0.2
        
        # Sample size recommendations
        recommended_n = self._calculate_required_sample_size(effect_size, 0.8, 0.05)
        
        return {
            'current_power': power,
            'effect_size_used': effect_size,
            'sample_sizes': {'n1': n1, 'n2': n2},
            'recommended_sample_size': recommended_n,
            'power_adequate': power >= 0.8
        }
    
    def _calculate_required_sample_size(self, effect_size: float, 
                                       desired_power: float = 0.8,
                                       alpha: float = 0.05) -> int:
        """Calculate required sample size for desired power"""
        # Simplified calculation for equal sample sizes
        if effect_size == 0:
            return 1000  # Large number for zero effect
        
        # Approximate formula for two-sample t-test
        z_alpha = 1.96  # For alpha = 0.05
        z_beta = 0.84   # For power = 0.8
        
        n = 2 * ((z_alpha + z_beta) / effect_size) ** 2
        return max(10, int(np.ceil(n)))
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """Interpret Cohen's d effect size"""
        if cohens_d < 0.2:
            return "negligible"
        elif cohens_d < 0.5:
            return "small"
        elif cohens_d < 0.8:
            return "medium"
        else:
            return "large"
    
    def _make_recommendation(self, p_value: float, cohens_d: float) -> str:
        """Make practical recommendation based on statistical results"""
        statistically_sig = p_value < self.config.significance_threshold
        practically_sig = abs(cohens_d) > self.config.effect_size_threshold
        
        if statistically_sig and practically_sig:
            return "Strong evidence for meaningful difference - recommend model with better performance"
        elif statistically_sig and not practically_sig:
            return "Statistically significant but small effect - consider practical constraints"
        elif not statistically_sig and practically_sig:
            return "Large effect but not statistically significant - collect more data"
        else:
            return "No meaningful difference detected - either model acceptable"
    
    def _approximate_t_cdf(self, t: float, df: float) -> float:
        """Approximate t-distribution CDF"""
        return 0.5 + 0.5 * np.tanh(t / np.sqrt(df + t**2))
    
    def _approximate_normal_cdf(self, z: float) -> float:
        """Approximate standard normal CDF"""
        return 0.5 * (1 + np.tanh(z * np.sqrt(2/np.pi)))


class AdversarialTester:
    """Adversarial robustness testing"""
    
    def __init__(self, config: AdversarialConfig):
        self.config = config
        
    def test_robustness(self, model_function: Callable, 
                       test_data: np.ndarray) -> Dict[str, Any]:
        """Test model robustness against various adversarial attacks"""
        
        logger.info("Starting adversarial robustness testing")
        
        results = {
            'baseline_performance': 0.0,
            'attack_results': {},
            'robustness_score': 0.0,
            'vulnerability_analysis': {}
        }
        
        # Baseline performance
        try:
            baseline_predictions = model_function(test_data)
            baseline_accuracy = self._calculate_accuracy(baseline_predictions, test_data)
            results['baseline_performance'] = baseline_accuracy
        except Exception as e:
            logger.error(f"Baseline evaluation failed: {e}")
            return {'error': str(e)}
        
        # Test each attack type
        for attack_type in self.config.attack_types:
            logger.info(f"Testing {attack_type} attacks")
            
            attack_results = []
            
            if attack_type == "gaussian_noise":
                attack_results = self._test_gaussian_noise(model_function, test_data)
            elif attack_type == "uniform_noise":
                attack_results = self._test_uniform_noise(model_function, test_data)
            elif attack_type == "signal_dropout":
                attack_results = self._test_signal_dropout(model_function, test_data)
            elif attack_type == "temporal_shift":
                attack_results = self._test_temporal_shift(model_function, test_data)
            
            results['attack_results'][attack_type] = attack_results
        
        # Compute overall robustness score
        results['robustness_score'] = self._compute_robustness_score(results['attack_results'])
        
        # Vulnerability analysis
        results['vulnerability_analysis'] = self._analyze_vulnerabilities(results['attack_results'])
        
        return results
    
    def _test_gaussian_noise(self, model_function: Callable, 
                           test_data: np.ndarray) -> List[Dict[str, Any]]:
        """Test against Gaussian noise attacks"""
        results = []
        
        for noise_level in self.config.noise_levels:
            # Add Gaussian noise
            noise = np.random.normal(0, noise_level, test_data.shape)
            adversarial_data = test_data + noise
            
            try:
                predictions = model_function(adversarial_data)
                accuracy = self._calculate_accuracy(predictions, test_data)
                
                results.append({
                    'noise_level': noise_level,
                    'accuracy': accuracy,
                    'performance_degradation': max(0, 1 - accuracy),
                    'attack_success_rate': max(0, 1 - accuracy) if accuracy < 0.5 else 0
                })
            except Exception as e:
                logger.warning(f"Gaussian noise test failed at level {noise_level}: {e}")
                results.append({
                    'noise_level': noise_level,
                    'error': str(e)
                })
        
        return results
    
    def _test_uniform_noise(self, model_function: Callable, 
                          test_data: np.ndarray) -> List[Dict[str, Any]]:
        """Test against uniform noise attacks"""
        results = []
        
        for noise_level in self.config.noise_levels:
            # Add uniform noise
            noise = np.random.uniform(-noise_level, noise_level, test_data.shape)
            adversarial_data = test_data + noise
            
            try:
                predictions = model_function(adversarial_data)
                accuracy = self._calculate_accuracy(predictions, test_data)
                
                results.append({
                    'noise_level': noise_level,
                    'accuracy': accuracy,
                    'performance_degradation': max(0, 1 - accuracy)
                })
            except Exception as e:
                results.append({
                    'noise_level': noise_level,
                    'error': str(e)
                })
        
        return results
    
    def _test_signal_dropout(self, model_function: Callable, 
                           test_data: np.ndarray) -> List[Dict[str, Any]]:
        """Test against signal dropout attacks"""
        results = []
        
        for dropout_rate in self.config.dropout_rates:
            # Apply dropout to random channels/features
            adversarial_data = test_data.copy()
            
            if adversarial_data.ndim >= 2:
                # Apply dropout to channels
                num_channels = adversarial_data.shape[-2] if adversarial_data.ndim == 3 else adversarial_data.shape[-1]
                dropout_mask = np.random.random(num_channels) > dropout_rate
                
                if adversarial_data.ndim == 3:
                    adversarial_data[:, ~dropout_mask, :] = 0
                else:
                    adversarial_data[:, ~dropout_mask] = 0
            
            try:
                predictions = model_function(adversarial_data)
                accuracy = self._calculate_accuracy(predictions, test_data)
                
                results.append({
                    'dropout_rate': dropout_rate,
                    'accuracy': accuracy,
                    'channels_dropped': int(np.sum(~dropout_mask)) if 'dropout_mask' in locals() else 0
                })
            except Exception as e:
                results.append({
                    'dropout_rate': dropout_rate,
                    'error': str(e)
                })
        
        return results
    
    def _test_temporal_shift(self, model_function: Callable, 
                           test_data: np.ndarray) -> List[Dict[str, Any]]:
        """Test against temporal shift attacks"""
        results = []
        
        if test_data.ndim < 3:
            # Cannot apply temporal shift to non-temporal data
            return [{'error': 'Temporal shift requires 3D data (samples, channels, time)'}]
        
        for shift in self.config.temporal_shifts:
            # Apply temporal shift
            adversarial_data = test_data.copy()
            
            if shift > 0 and shift < test_data.shape[-1]:
                # Right shift (delay)
                adversarial_data[:, :, shift:] = test_data[:, :, :-shift]
                adversarial_data[:, :, :shift] = 0  # Zero padding
            elif shift < 0 and abs(shift) < test_data.shape[-1]:
                # Left shift (advance)
                adversarial_data[:, :, :shift] = test_data[:, :, abs(shift):]
                adversarial_data[:, :, shift:] = 0  # Zero padding
            
            try:
                predictions = model_function(adversarial_data)
                accuracy = self._calculate_accuracy(predictions, test_data)
                
                results.append({
                    'temporal_shift': shift,
                    'accuracy': accuracy,
                    'shift_direction': 'delay' if shift > 0 else 'advance'
                })
            except Exception as e:
                results.append({
                    'temporal_shift': shift,
                    'error': str(e)
                })
        
        return results
    
    def _calculate_accuracy(self, predictions: np.ndarray, 
                          reference_data: np.ndarray) -> float:
        """Calculate accuracy (simplified simulation)"""
        # In real implementation, this would compare predictions to ground truth
        # Here we simulate accuracy based on prediction confidence/consistency
        
        if predictions.ndim == 1:
            # Classification predictions
            confidence = np.mean(np.abs(predictions - 0.5)) * 2  # Convert to 0-1 scale
        else:
            # Multi-dimensional predictions
            confidence = np.mean(np.std(predictions, axis=-1))  # Consistency measure
        
        # Simulate accuracy based on confidence
        base_accuracy = 0.8
        noise_factor = np.random.normal(0, 0.05)
        
        accuracy = max(0.0, min(1.0, base_accuracy * confidence + noise_factor))
        return accuracy
    
    def _compute_robustness_score(self, attack_results: Dict[str, List[Dict]]) -> float:
        """Compute overall robustness score"""
        all_accuracies = []
        
        for attack_type, results in attack_results.items():
            for result in results:
                if 'accuracy' in result:
                    all_accuracies.append(result['accuracy'])
        
        if not all_accuracies:
            return 0.0
        
        # Robustness score is the minimum accuracy across all attacks
        return float(min(all_accuracies))
    
    def _analyze_vulnerabilities(self, attack_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """Analyze model vulnerabilities"""
        vulnerabilities = {
            'most_vulnerable_to': None,
            'least_vulnerable_to': None,
            'vulnerability_ranking': [],
            'critical_thresholds': {}
        }
        
        attack_scores = {}
        
        for attack_type, results in attack_results.items():
            accuracies = [r['accuracy'] for r in results if 'accuracy' in r]
            if accuracies:
                min_accuracy = min(accuracies)
                attack_scores[attack_type] = min_accuracy
        
        if attack_scores:
            # Rank vulnerabilities
            sorted_attacks = sorted(attack_scores.items(), key=lambda x: x[1])
            
            vulnerabilities['most_vulnerable_to'] = sorted_attacks[0][0]
            vulnerabilities['least_vulnerable_to'] = sorted_attacks[-1][0]
            vulnerabilities['vulnerability_ranking'] = [
                {'attack': attack, 'min_accuracy': score}
                for attack, score in sorted_attacks
            ]
        
        return vulnerabilities


class PerformanceProfiler:
    """Real-time performance monitoring and profiling"""
    
    def __init__(self):
        self.metrics_buffer = deque(maxlen=1000)
        self.monitoring_active = False
        self.monitor_thread = None
        
    def start_monitoring(self, model_function: Callable, 
                        sample_data: np.ndarray):
        """Start real-time performance monitoring"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(model_function, sample_data),
            daemon=True
        )
        self.monitor_thread.start()
        
        logger.info("Performance monitoring started")
    
    def stop_monitoring(self):
        """Stop performance monitoring"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join()
        
        logger.info("Performance monitoring stopped")
    
    def _monitor_loop(self, model_function: Callable, sample_data: np.ndarray):
        """Monitoring loop running in separate thread"""
        while self.monitoring_active:
            try:
                # Measure inference time
                start_time = time.time()
                predictions = model_function(sample_data)
                inference_time = time.time() - start_time
                
                # Collect metrics
                metrics = {
                    'timestamp': time.time(),
                    'inference_time_ms': inference_time * 1000,
                    'throughput_samples_per_sec': len(sample_data) / inference_time,
                    'prediction_shape': predictions.shape if hasattr(predictions, 'shape') else None,
                    'memory_usage_mb': self._estimate_memory_usage(predictions)
                }
                
                self.metrics_buffer.append(metrics)
                
                # Sleep to avoid overwhelming the system
                time.sleep(1.0)
                
            except Exception as e:
                logger.warning(f"Monitoring error: {e}")
                time.sleep(1.0)
    
    def _estimate_memory_usage(self, data: Any) -> float:
        """Estimate memory usage in MB"""
        if hasattr(data, 'nbytes'):
            return data.nbytes / (1024 * 1024)
        elif isinstance(data, (list, tuple)):
            return sum(self._estimate_memory_usage(item) for item in data)
        else:
            return 1.0  # Default estimate
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance monitoring summary"""
        if not self.metrics_buffer:
            return {'error': 'No performance data available'}
        
        # Convert to arrays for analysis
        inference_times = np.array([m['inference_time_ms'] for m in self.metrics_buffer])
        throughputs = np.array([m['throughput_samples_per_sec'] for m in self.metrics_buffer])
        memory_usage = np.array([m['memory_usage_mb'] for m in self.metrics_buffer])
        
        summary = {
            'monitoring_duration_seconds': self.metrics_buffer[-1]['timestamp'] - self.metrics_buffer[0]['timestamp'],
            'num_measurements': len(self.metrics_buffer),
            'inference_time_ms': {
                'mean': float(np.mean(inference_times)),
                'median': float(np.median(inference_times)),
                'p95': float(np.percentile(inference_times, 95)),
                'p99': float(np.percentile(inference_times, 99)),
                'min': float(np.min(inference_times)),
                'max': float(np.max(inference_times))
            },
            'throughput_samples_per_sec': {
                'mean': float(np.mean(throughputs)),
                'median': float(np.median(throughputs)),
                'min': float(np.min(throughputs)),
                'max': float(np.max(throughputs))
            },
            'memory_usage_mb': {
                'mean': float(np.mean(memory_usage)),
                'max': float(np.max(memory_usage)),
                'min': float(np.min(memory_usage))
            }
        }
        
        return summary


class BenchmarkFramework:
    """Main benchmarking framework coordinating all testing components"""
    
    def __init__(self, config: Optional[BenchmarkConfig] = None):
        self.config = config or BenchmarkConfig()
        self.statistical_analyzer = StatisticalAnalyzer(StatisticalTestConfig())
        self.adversarial_tester = AdversarialTester(AdversarialConfig())
        self.performance_profiler = PerformanceProfiler()
        self.benchmark_history = []
        
    def comprehensive_benchmark(self, 
                              models: Dict[str, Callable],
                              test_data: np.ndarray,
                              ground_truth: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """Run comprehensive benchmark suite"""
        
        logger.info(f"Starting comprehensive benchmark of {len(models)} models")
        
        benchmark_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
        
        results = {
            'benchmark_id': benchmark_id,
            'timestamp': time.time(),
            'models': list(models.keys()),
            'config': {
                'metrics': [m.value for m in self.config.metrics],
                'num_trials': self.config.num_trials,
                'confidence_level': self.config.confidence_level
            },
            'individual_results': {},
            'model_comparisons': {},
            'adversarial_results': {},
            'performance_profiles': {},
            'recommendations': {}
        }
        
        # Individual model benchmarking
        for model_name, model_function in models.items():
            logger.info(f"Benchmarking model: {model_name}")
            
            model_results = {}
            
            # Standard benchmarks for each metric
            for metric in self.config.metrics:
                benchmark_result = BenchmarkResult(model_name, metric)
                
                # Run multiple trials
                for trial in range(self.config.num_trials):
                    try:
                        score = self._evaluate_metric(model_function, test_data, metric, ground_truth)
                        benchmark_result.add_score(score, {'trial': trial})
                    except Exception as e:
                        logger.warning(f"Trial {trial} failed for {model_name}/{metric.value}: {e}")
                
                model_results[metric.value] = benchmark_result.to_dict()
            
            # Adversarial testing
            if BenchmarkMetric.ROBUSTNESS in self.config.metrics:
                logger.info(f"Adversarial testing for {model_name}")
                adversarial_results = self.adversarial_tester.test_robustness(
                    model_function, test_data
                )
                results['adversarial_results'][model_name] = adversarial_results
            
            # Performance profiling
            logger.info(f"Performance profiling for {model_name}")
            self.performance_profiler.start_monitoring(model_function, test_data[:10])
            time.sleep(5)  # Monitor for 5 seconds
            self.performance_profiler.stop_monitoring()
            
            performance_profile = self.performance_profiler.get_performance_summary()
            results['performance_profiles'][model_name] = performance_profile
            
            results['individual_results'][model_name] = model_results
        
        # Model comparisons
        if len(models) > 1:
            results['model_comparisons'] = self._compare_all_models(results['individual_results'])
        
        # Generate recommendations
        results['recommendations'] = self._generate_recommendations(results)
        
        # Store in history
        self.benchmark_history.append(results)
        
        return results
    
    def _evaluate_metric(self, model_function: Callable, test_data: np.ndarray,
                        metric: BenchmarkMetric, ground_truth: Optional[np.ndarray]) -> float:
        """Evaluate a specific metric"""
        
        if metric == BenchmarkMetric.LATENCY:
            # Measure inference latency
            start_time = time.time()
            predictions = model_function(test_data)
            latency_ms = (time.time() - start_time) * 1000
            return latency_ms
        
        elif metric == BenchmarkMetric.THROUGHPUT:
            # Measure throughput
            start_time = time.time()
            predictions = model_function(test_data)
            duration = time.time() - start_time
            throughput = len(test_data) / duration if duration > 0 else 0
            return throughput
        
        elif metric == BenchmarkMetric.ACCURACY:
            # Simulate accuracy evaluation
            predictions = model_function(test_data)
            
            if ground_truth is not None:
                # Calculate real accuracy if ground truth available
                if predictions.shape == ground_truth.shape:
                    accuracy = np.mean(np.abs(predictions - ground_truth) < 0.1)
                else:
                    accuracy = 0.7 + np.random.normal(0, 0.1)  # Simulate
            else:
                # Simulate accuracy based on prediction consistency
                accuracy = 0.8 + np.random.normal(0, 0.05)
            
            return max(0.0, min(1.0, accuracy))
        
        elif metric == BenchmarkMetric.MEMORY_USAGE:
            # Estimate memory usage
            predictions = model_function(test_data)
            memory_mb = self.performance_profiler._estimate_memory_usage(predictions)
            return memory_mb
        
        else:
            # Default simulation for other metrics
            return np.random.uniform(0.5, 0.95)
    
    def _compare_all_models(self, individual_results: Dict[str, Dict]) -> Dict[str, Any]:
        """Compare all models pairwise"""
        comparisons = {}
        model_names = list(individual_results.keys())
        
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names[i+1:], i+1):
                comparison_key = f"{model1}_vs_{model2}"
                comparisons[comparison_key] = {}
                
                # Compare each metric
                for metric in individual_results[model1].keys():
                    if metric in individual_results[model2]:
                        # Create BenchmarkResult objects for comparison
                        result1 = BenchmarkResult(model1, BenchmarkMetric(metric))
                        result1.raw_scores = individual_results[model1][metric]['raw_scores']
                        
                        result2 = BenchmarkResult(model2, BenchmarkMetric(metric))  
                        result2.raw_scores = individual_results[model2][metric]['raw_scores']
                        
                        comparison = self.statistical_analyzer.compare_models(result1, result2)
                        comparisons[comparison_key][metric] = comparison
        
        return comparisons
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate actionable recommendations"""
        recommendations = {
            'best_overall_model': None,
            'metric_specific_winners': {},
            'performance_insights': [],
            'improvement_suggestions': [],
            'robustness_warnings': []
        }
        
        # Find metric-specific winners
        for metric in [m.value for m in self.config.metrics]:
            metric_scores = {}
            
            for model_name, model_results in results['individual_results'].items():
                if metric in model_results and model_results[metric]['statistics']:
                    metric_scores[model_name] = model_results[metric]['statistics']['mean']
            
            if metric_scores:
                if metric in ['latency', 'memory_usage']:
                    # Lower is better
                    winner = min(metric_scores.items(), key=lambda x: x[1])
                else:
                    # Higher is better
                    winner = max(metric_scores.items(), key=lambda x: x[1])
                
                recommendations['metric_specific_winners'][metric] = {
                    'model': winner[0],
                    'score': winner[1]
                }
        
        # Overall best model (simplified scoring)
        overall_scores = {}
        for model_name in results['individual_results'].keys():
            score = 0
            count = 0
            
            for metric in ['accuracy', 'throughput']:
                if (metric in results['individual_results'][model_name] and 
                    results['individual_results'][model_name][metric]['statistics']):
                    score += results['individual_results'][model_name][metric]['statistics']['mean']
                    count += 1
            
            # Penalize for high latency and memory usage
            for metric in ['latency', 'memory_usage']:
                if (metric in results['individual_results'][model_name] and 
                    results['individual_results'][model_name][metric]['statistics']):
                    penalty = results['individual_results'][model_name][metric]['statistics']['mean']
                    score -= penalty * 0.01  # Small penalty weight
                    count += 1
            
            if count > 0:
                overall_scores[model_name] = score / count
        
        if overall_scores:
            best_model = max(overall_scores.items(), key=lambda x: x[1])
            recommendations['best_overall_model'] = {
                'model': best_model[0],
                'score': best_model[1]
            }
        
        # Performance insights
        for model_name, profile in results['performance_profiles'].items():
            if 'error' not in profile:
                latency_p95 = profile['inference_time_ms']['p95']
                throughput_mean = profile['throughput_samples_per_sec']['mean']
                
                if latency_p95 > 1000:  # > 1 second
                    recommendations['improvement_suggestions'].append(
                        f"{model_name}: High latency ({latency_p95:.0f}ms) - consider model optimization"
                    )
                
                if throughput_mean < 10:  # < 10 samples/sec
                    recommendations['improvement_suggestions'].append(
                        f"{model_name}: Low throughput ({throughput_mean:.1f} samples/sec) - consider batching or hardware acceleration"
                    )
        
        # Robustness warnings
        for model_name, adversarial_results in results['adversarial_results'].items():
            if 'robustness_score' in adversarial_results:
                robustness = adversarial_results['robustness_score']
                if robustness < 0.7:
                    recommendations['robustness_warnings'].append(
                        f"{model_name}: Low robustness score ({robustness:.2f}) - vulnerable to adversarial attacks"
                    )
        
        return recommendations
    
    def export_results(self, results: Dict[str, Any], 
                      filepath: str, format: str = "json"):
        """Export benchmark results"""
        
        if format == "json":
            with open(filepath, 'w') as f:
                json.dump(results, f, indent=2, default=str)
        
        logger.info(f"Benchmark results exported to {filepath}")


# Testing and demonstration
def run_advanced_benchmarking_tests():
    """Run comprehensive benchmarking tests"""
    
    print("ðŸ”¬ ADVANCED BENCHMARKING FRAMEWORK TESTS")
    print("="*55)
    
    # Create test models
    def model_a(data):
        time.sleep(0.1)  # Simulate processing time
        return np.random.uniform(0, 1, (len(data), 10))
    
    def model_b(data):
        time.sleep(0.05)  # Faster model
        return np.random.uniform(0.2, 0.8, (len(data), 10))
    
    def model_c(data):
        time.sleep(0.15)  # Slower but potentially more accurate
        return np.random.uniform(0.1, 0.9, (len(data), 10))
    
    models = {
        'ModelA': model_a,
        'ModelB': model_b,
        'ModelC': model_c
    }
    
    # Generate test data
    np.random.seed(42)
    test_data = np.random.randn(100, 32, 256)  # 100 samples, 32 channels, 256 timepoints
    ground_truth = np.random.uniform(0, 1, (100, 10))
    
    # Initialize framework
    config = BenchmarkConfig(
        metrics=[
            BenchmarkMetric.ACCURACY,
            BenchmarkMetric.LATENCY, 
            BenchmarkMetric.THROUGHPUT,
            BenchmarkMetric.ROBUSTNESS
        ],
        num_trials=5,  # Reduced for testing
        parallel_execution=True
    )
    
    framework = BenchmarkFramework(config)
    
    # Run comprehensive benchmark
    print("\nðŸš€ Running comprehensive benchmark...")
    results = framework.comprehensive_benchmark(models, test_data, ground_truth)
    
    print("\nðŸ“Š BENCHMARK RESULTS:")
    print("-" * 40)
    
    # Individual results summary
    for model_name, model_results in results['individual_results'].items():
        print(f"\nâœ… {model_name}:")
        
        for metric, metric_results in model_results.items():
            if 'statistics' in metric_results and metric_results['statistics']:
                stats = metric_results['statistics']
                mean = stats['mean']
                std = stats['std']
                print(f"   {metric}: {mean:.3f} Â± {std:.3f}")
    
    # Model comparisons
    if results['model_comparisons']:
        print(f"\nðŸ” MODEL COMPARISONS:")
        print("-" * 40)
        
        for comparison_key, comparison_results in results['model_comparisons'].items():
            print(f"\n{comparison_key}:")
            
            for metric, comparison in comparison_results.items():
                if 'practical_significance' in comparison:
                    practical = comparison['practical_significance']
                    stat_sig = practical['statistically_significant']
                    practical_sig = practical['practically_significant']
                    recommendation = practical['recommendation']
                    
                    print(f"   {metric}:")
                    print(f"     Statistical: {'âœ…' if stat_sig else 'âŒ'}")
                    print(f"     Practical: {'âœ…' if practical_sig else 'âŒ'}")
                    print(f"     Recommendation: {recommendation}")
    
    # Adversarial results
    print(f"\nðŸ›¡ï¸ ADVERSARIAL ROBUSTNESS:")
    print("-" * 40)
    
    for model_name, adversarial_results in results['adversarial_results'].items():
        if 'robustness_score' in adversarial_results:
            robustness = adversarial_results['robustness_score']
            print(f"{model_name}: {robustness:.3f}")
            
            if 'vulnerability_analysis' in adversarial_results:
                vuln = adversarial_results['vulnerability_analysis']
                if vuln.get('most_vulnerable_to'):
                    print(f"   Most vulnerable to: {vuln['most_vulnerable_to']}")
    
    # Performance profiles
    print(f"\nâš¡ PERFORMANCE PROFILES:")
    print("-" * 40)
    
    for model_name, profile in results['performance_profiles'].items():
        if 'error' not in profile:
            latency = profile['inference_time_ms']['mean']
            throughput = profile['throughput_samples_per_sec']['mean']
            memory = profile['memory_usage_mb']['mean']
            
            print(f"{model_name}:")
            print(f"   Latency: {latency:.1f}ms")
            print(f"   Throughput: {throughput:.1f} samples/sec")
            print(f"   Memory: {memory:.1f}MB")
    
    # Recommendations
    print(f"\nðŸ’¡ RECOMMENDATIONS:")
    print("-" * 40)
    
    recommendations = results['recommendations']
    
    if recommendations['best_overall_model']:
        best = recommendations['best_overall_model']
        print(f"Best Overall: {best['model']} (score: {best['score']:.3f})")
    
    if recommendations['improvement_suggestions']:
        print("\nImprovement Suggestions:")
        for suggestion in recommendations['improvement_suggestions']:
            print(f"  â€¢ {suggestion}")
    
    if recommendations['robustness_warnings']:
        print("\nRobustness Warnings:")
        for warning in recommendations['robustness_warnings']:
            print(f"  âš ï¸  {warning}")
    
    print("\nâœ… Advanced benchmarking tests completed successfully!")
    
    return results


if __name__ == "__main__":
    run_advanced_benchmarking_tests()